"""
NLP(ìì—°ì–´ ì²˜ë¦¬) ê´€ë ¨ ë¼ìš°í„°
"""
from fastapi import APIRouter, HTTPException, Query, Body
from fastapi.responses import HTMLResponse, JSONResponse
from typing import List, Dict, Any, Optional
from pathlib import Path
from pydantic import BaseModel, Field
import sys
import base64
import io

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from app.nlp.nlp_service import NLPService
from common.utils import create_response, create_error_response
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/nlp", tags=["nlp"])

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì‹±ê¸€í†¤ íŒ¨í„´)
_service_instance: Optional[NLPService] = None


def get_service() -> NLPService:
    """NLPService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _service_instance
    if _service_instance is None:
        _service_instance = NLPService()
    return _service_instance


# Pydantic ëª¨ë¸ ì •ì˜
class TextInput(BaseModel):
    """í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë¸"""
    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=1)
    name: Optional[str] = Field("Document", description="ë¬¸ì„œ ì´ë¦„")
    tokenize_method: Optional[str] = Field("regexp", description="í† í°í™” ë°©ë²•: word, sentence, regexp")


class TokenizeInput(BaseModel):
    """í† í°í™” ì…ë ¥ ëª¨ë¸"""
    text: str = Field(..., description="í† í°í™”í•  í…ìŠ¤íŠ¸", min_length=1)
    method: Optional[str] = Field("word", description="í† í°í™” ë°©ë²•: word, sentence, regexp")


class StemInput(BaseModel):
    """ì–´ê°„ ì¶”ì¶œ ì…ë ¥ ëª¨ë¸"""
    words: List[str] = Field(..., description="ì–´ê°„ ì¶”ì¶œí•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸", min_items=1)
    method: Optional[str] = Field("porter", description="ì–´ê°„ ì¶”ì¶œ ë°©ë²•: porter, lancaster")


class LemmatizeInput(BaseModel):
    """ì›í˜• ë³µì› ì…ë ¥ ëª¨ë¸"""
    words: List[str] = Field(..., description="ì›í˜• ë³µì›í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸", min_items=1)
    pos: Optional[str] = Field(None, description="í’ˆì‚¬: v(ë™ì‚¬), n(ëª…ì‚¬), a(í˜•ìš©ì‚¬), r(ë¶€ì‚¬)")


class POSTagInput(BaseModel):
    """í’ˆì‚¬ íƒœê¹… ì…ë ¥ ëª¨ë¸"""
    text: str = Field(..., description="í’ˆì‚¬ íƒœê¹…í•  í…ìŠ¤íŠ¸", min_length=1)
    filter_pos: Optional[str] = Field(None, description="í•„í„°ë§í•  í’ˆì‚¬ íƒœê·¸ (ì˜ˆ: NN, NNP, VB)")


@router.get("/")
async def nlp_root():
    """NLP ì„œë¹„ìŠ¤ ë£¨íŠ¸"""
    return create_response(
        data={"service": "mlservice", "module": "nlp", "status": "running"},
        message="NLP Service is running"
    )


@router.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        service = get_service()
        files = service.corpus_manager.get_available_files()
        return create_response(
            data={"status": "healthy", "service": "nlp", "available_corpus_files": len(files)},
            message="NLP service is healthy"
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Service unhealthy: {str(e)}")


@router.get("/corpus/files")
async def get_corpus_files():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë§ë­‰ì¹˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    
    NLTK Gutenberg ë§ë­‰ì¹˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        service = get_service()
        files = service.corpus_manager.get_available_files()
        
        return create_response(
            data={
                "count": len(files),
                "files": files
            },
            message="ë§ë­‰ì¹˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ"
        )
    except Exception as e:
        logger.error(f"ë§ë­‰ì¹˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë§ë­‰ì¹˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/corpus/preview")
async def preview_corpus(
    file_id: str = Query("austen-emma.txt", description="ë§ë­‰ì¹˜ íŒŒì¼ ID"),
    length: int = Query(1000, description="ë¯¸ë¦¬ë³´ê¸° ê¸¸ì´", ge=100, le=10000)
):
    """
    ë§ë­‰ì¹˜ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
    
    ì§€ì •ëœ ë§ë­‰ì¹˜ íŒŒì¼ì˜ ì¼ë¶€ë¶„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        service = get_service()
        preview = service.corpus_manager.preview_corpus(file_id, length)
        
        return create_response(
            data={
                "file_id": file_id,
                "preview_length": len(preview),
                "preview": preview
            },
            message="ë§ë­‰ì¹˜ ë¯¸ë¦¬ë³´ê¸° ì¡°íšŒ ì™„ë£Œ"
        )
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    except Exception as e:
        logger.error(f"ë§ë­‰ì¹˜ ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë§ë­‰ì¹˜ ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.post("/tokenize")
async def tokenize_text(input_data: TokenizeInput):
    """
    í…ìŠ¤íŠ¸ í† í°í™”
    
    ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•œ ë°©ë²•ìœ¼ë¡œ í† í°í™”í•©ë‹ˆë‹¤.
    - word: ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”
    - sentence: ë¬¸ì¥ ë‹¨ìœ„ í† í°í™”
    - regexp: ì •ê·œí‘œí˜„ì‹ í† í°í™”
    """
    try:
        service = get_service()
        
        if input_data.method == "word":
            tokens = service.tokenizer.tokenize_words(input_data.text)
        elif input_data.method == "sentence":
            tokens = service.tokenizer.tokenize_sentences(input_data.text)
        elif input_data.method == "regexp":
            tokens = service.tokenizer.tokenize_regexp(input_data.text)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í† í°í™” ë°©ë²•: {input_data.method}")
        
        return create_response(
            data={
                "method": input_data.method,
                "token_count": len(tokens),
                "tokens": tokens
            },
            message="í† í°í™” ì™„ë£Œ"
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"í† í°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"í† í°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.post("/stem")
async def stem_words(input_data: StemInput):
    """
    ì–´ê°„ ì¶”ì¶œ (Stemming)
    
    ë‹¨ì–´ì˜ ì ‘ë¯¸ì‚¬ë‚˜ ì–´ë¯¸ë¥¼ ì œê±°í•˜ì—¬ ê¸°ë³¸í˜•ì„ ì°¾ìŠµë‹ˆë‹¤.
    - porter: Porter Stemmer (ë³´í¸ì )
    - lancaster: Lancaster Stemmer (ë” ê³µê²©ì )
    """
    try:
        service = get_service()
        
        if input_data.method == "porter":
            stems = service.morphology.stem_porter(input_data.words)
        elif input_data.method == "lancaster":
            stems = service.morphology.stem_lancaster(input_data.words)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–´ê°„ ì¶”ì¶œ ë°©ë²•: {input_data.method}")
        
        # ì›ë³¸ê³¼ ê²°ê³¼ë¥¼ ë§¤í•‘
        result_pairs = [{"original": orig, "stem": stem} 
                       for orig, stem in zip(input_data.words, stems)]
        
        return create_response(
            data={
                "method": input_data.method,
                "count": len(stems),
                "results": result_pairs
            },
            message="ì–´ê°„ ì¶”ì¶œ ì™„ë£Œ"
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ì–´ê°„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì–´ê°„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.post("/lemmatize")
async def lemmatize_words(input_data: LemmatizeInput):
    """
    ì›í˜• ë³µì› (Lemmatizing)
    
    ë‹¨ì–´ë¥¼ ì‚¬ì „í˜•ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤. í’ˆì‚¬ë¥¼ ì§€ì •í•˜ë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - pos: v(ë™ì‚¬), n(ëª…ì‚¬), a(í˜•ìš©ì‚¬), r(ë¶€ì‚¬)
    """
    try:
        service = get_service()
        lemmas = service.morphology.lemmatize(input_data.words, input_data.pos)
        
        # ì›ë³¸ê³¼ ê²°ê³¼ë¥¼ ë§¤í•‘
        result_pairs = [{"original": orig, "lemma": lemma} 
                       for orig, lemma in zip(input_data.words, lemmas)]
        
        return create_response(
            data={
                "pos": input_data.pos or "auto",
                "count": len(lemmas),
                "results": result_pairs
            },
            message="ì›í˜• ë³µì› ì™„ë£Œ"
        )
    except Exception as e:
        logger.error(f"ì›í˜• ë³µì› ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì›í˜• ë³µì› ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.post("/pos-tag")
async def pos_tag_text(input_data: POSTagInput):
    """
    í’ˆì‚¬ íƒœê¹… (POS Tagging)
    
    í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ê° í† í°ì— í’ˆì‚¬ë¥¼ ë¶€ì°©í•©ë‹ˆë‹¤.
    - filter_posë¥¼ ì§€ì •í•˜ë©´ í•´ë‹¹ í’ˆì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    
    ì£¼ìš” í’ˆì‚¬ íƒœê·¸:
    - NN: ëª…ì‚¬(ë‹¨ìˆ˜)
    - NNP: ê³ ìœ ëª…ì‚¬(ë‹¨ìˆ˜)
    - VB: ë™ì‚¬
    - JJ: í˜•ìš©ì‚¬
    - RB: ë¶€ì‚¬
    """
    try:
        service = get_service()
        
        # í† í°í™”
        tokens = service.tokenizer.tokenize_words(input_data.text)
        
        # í’ˆì‚¬ íƒœê¹…
        tagged = service.pos_tagger.tag(tokens)
        
        # í•„í„°ë§ (ì„ íƒì )
        if input_data.filter_pos:
            filtered_tokens = service.pos_tagger.filter_by_pos(tagged, input_data.filter_pos)
            return create_response(
                data={
                    "total_tokens": len(tokens),
                    "tagged_tokens": tagged,
                    "filter_pos": input_data.filter_pos,
                    "filtered_count": len(filtered_tokens),
                    "filtered_tokens": filtered_tokens
                },
                message="í’ˆì‚¬ íƒœê¹… ë° í•„í„°ë§ ì™„ë£Œ"
            )
        
        return create_response(
            data={
                "total_tokens": len(tokens),
                "tagged_tokens": tagged
            },
            message="í’ˆì‚¬ íƒœê¹… ì™„ë£Œ"
        )
    except Exception as e:
        logger.error(f"í’ˆì‚¬ íƒœê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"í’ˆì‚¬ íƒœê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.post("/analyze")
async def analyze_text(input_data: TextInput):
    """
    í…ìŠ¤íŠ¸ ì¢…í•© ë¶„ì„
    
    ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    - í† í°í™”
    - ë¹ˆë„ ë¶„ì„
    - ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ ì¶”ì¶œ
    - í†µê³„ ì •ë³´
    """
    try:
        service = get_service()
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ê¸° ìƒì„±
        analyzer = service.create_analyzer(
            text=input_data.text,
            name=input_data.name,
            tokenize_method=input_data.tokenize_method
        )
        
        # ë¶„ì„ ìˆ˜í–‰
        freq_dist = analyzer.get_freq_dist()
        most_common = analyzer.most_common(20)
        
        return create_response(
            data={
                "document_name": input_data.name,
                "total_tokens": len(analyzer.tokens),
                "unique_tokens": len(freq_dist),
                "most_common_words": [
                    {"word": word, "count": count}
                    for word, count in most_common
                ],
                "lexical_diversity": round(len(freq_dist) / len(analyzer.tokens), 4) if analyzer.tokens else 0
            },
            message="í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ"
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/corpus/analyze")
async def analyze_corpus(
    file_id: str = Query("austen-emma.txt", description="ë¶„ì„í•  ë§ë­‰ì¹˜ íŒŒì¼ ID"),
    top_n: int = Query(20, description="ìƒìœ„ Nê°œ ë‹¨ì–´ ì¶”ì¶œ", ge=5, le=100)
):
    """
    ë§ë­‰ì¹˜ ì „ì²´ ë¶„ì„
    
    ì§€ì •ëœ ë§ë­‰ì¹˜ íŒŒì¼ì— ëŒ€í•´ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - í† í° í†µê³„
    - ë¹ˆë„ ë¶„ì„
    - ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
    """
    try:
        service = get_service()
        
        # ë§ë­‰ì¹˜ ë¶„ì„
        result = service.analyze_corpus(file_id)
        
        # ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
        stopwords = ["Mr.", "Mrs.", "Miss", "Mr", "Mrs", "Dear"]
        proper_nouns_fd = result['analyzer'].filter_proper_nouns(stopwords)
        proper_nouns_top = proper_nouns_fd.most_common(top_n)
        
        return create_response(
            data={
                "file_id": result['file_id'],
                "total_tokens": result['total_tokens'],
                "unique_tokens": result['unique_tokens'],
                "lexical_diversity": round(result['unique_tokens'] / result['total_tokens'], 4),
                "most_common_words": [
                    {"word": word, "count": count}
                    for word, count in result['most_common_words']
                ],
                "proper_nouns_top": [
                    {"word": word, "count": count}
                    for word, count in proper_nouns_top
                ]
            },
            message="ë§ë­‰ì¹˜ ë¶„ì„ ì™„ë£Œ"
        )
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    except Exception as e:
        logger.error(f"ë§ë­‰ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë§ë­‰ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/wordcloud", response_class=HTMLResponse)
async def generate_wordcloud(
    file_id: str = Query("austen-emma.txt", description="ë¶„ì„í•  ë§ë­‰ì¹˜ íŒŒì¼ ID"),
    width: int = Query(1000, description="ì›Œë“œí´ë¼ìš°ë“œ ë„ˆë¹„", ge=400, le=2000),
    height: int = Query(600, description="ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´", ge=300, le=1500),
    background_color: str = Query("white", description="ë°°ê²½ìƒ‰"),
    filter_type: str = Query("proper_nouns", description="í•„í„° íƒ€ì…: all, proper_nouns")
):
    """
    ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    
    ë§ë­‰ì¹˜ì˜ ë‹¨ì–´ ë¹ˆë„ë¥¼ ì‹œê°í™”í•œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ HTMLë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    - filter_type='all': ëª¨ë“  ë‹¨ì–´
    - filter_type='proper_nouns': ê³ ìœ ëª…ì‚¬ë§Œ
    """
    try:
        service = get_service()
        
        # ë§ë­‰ì¹˜ ë¶„ì„
        result = service.analyze_corpus(file_id)
        
        # í•„í„°ë§
        if filter_type == "proper_nouns":
            stopwords = ["Mr.", "Mrs.", "Miss", "Mr", "Mrs", "Dear"]
            freq_dist = result['analyzer'].filter_proper_nouns(stopwords)
            title = f"{file_id} - ê³ ìœ ëª…ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ"
        else:
            freq_dist = result['freq_dist']
            title = f"{file_id} - ì „ì²´ ë‹¨ì–´ ì›Œë“œí´ë¼ìš°ë“œ"
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        import matplotlib
        matplotlib.use('Agg')  # GUI ì—†ì´ ì‚¬ìš©
        import matplotlib.pyplot as plt
        
        wc = service.visualizer.generate_wordcloud(
            freq_dist,
            width=width,
            height=height,
            background_color=background_color,
            show=False
        )
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        buf = io.BytesIO()
        plt.figure(figsize=(width/100, height/100))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis("off")
        plt.tight_layout(pad=0)
        plt.savefig(buf, format='png', bbox_inches='tight', dpi=100)
        plt.close()
        
        buf.seek(0)
        img_base64 = base64.b64encode(buf.read()).decode('utf-8')
        
        # HTML ìƒì„±
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>{title}</title>
            <meta charset="utf-8">
            <style>
                body {{
                    font-family: 'Segoe UI', Arial, sans-serif;
                    margin: 0;
                    padding: 20px;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    min-height: 100vh;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background-color: white;
                    padding: 30px;
                    border-radius: 12px;
                    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
                }}
                h1 {{
                    color: #333;
                    text-align: center;
                    margin-bottom: 10px;
                    font-size: 2em;
                }}
                .subtitle {{
                    text-align: center;
                    color: #666;
                    margin-bottom: 30px;
                    font-size: 1.1em;
                }}
                .image-container {{
                    text-align: center;
                    margin: 30px 0;
                    background-color: #f9f9f9;
                    padding: 20px;
                    border-radius: 8px;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                    border-radius: 8px;
                    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                }}
                .info {{
                    margin-top: 30px;
                    padding: 20px;
                    background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
                    border-radius: 8px;
                    border-left: 4px solid #667eea;
                }}
                .info h2 {{
                    color: #667eea;
                    margin-top: 0;
                    font-size: 1.3em;
                }}
                .info ul {{
                    color: #555;
                    line-height: 1.8;
                }}
                .stats {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 15px;
                    margin-top: 20px;
                }}
                .stat-card {{
                    background-color: white;
                    padding: 15px;
                    border-radius: 8px;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.08);
                }}
                .stat-label {{
                    color: #888;
                    font-size: 0.9em;
                    margin-bottom: 5px;
                }}
                .stat-value {{
                    color: #333;
                    font-size: 1.5em;
                    font-weight: bold;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ“Š {title}</h1>
                <p class="subtitle">NLTK ìì—°ì–´ ì²˜ë¦¬ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”</p>
                
                <div class="stats">
                    <div class="stat-card">
                        <div class="stat-label">ì´ í† í° ìˆ˜</div>
                        <div class="stat-value">{result['total_tokens']:,}</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-label">ê³ ìœ  í† í° ìˆ˜</div>
                        <div class="stat-value">{result['unique_tokens']:,}</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-label">ì–´íœ˜ ë‹¤ì–‘ì„±</div>
                        <div class="stat-value">{result['unique_tokens'] / result['total_tokens']:.3f}</div>
                    </div>
                </div>
                
                <div class="image-container">
                    <img src="data:image/png;base64,{img_base64}" alt="ì›Œë“œí´ë¼ìš°ë“œ" />
                </div>
                
                <div class="info">
                    <h2>â„¹ï¸ ì›Œë“œí´ë¼ìš°ë“œ ì •ë³´</h2>
                    <ul>
                        <li><strong>íŒŒì¼:</strong> {file_id}</li>
                        <li><strong>í•„í„° íƒ€ì…:</strong> {"ê³ ìœ ëª…ì‚¬ë§Œ" if filter_type == "proper_nouns" else "ì „ì²´ ë‹¨ì–´"}</li>
                        <li><strong>í¬ê¸°:</strong> {width} Ã— {height} px</li>
                        <li><strong>ë°°ê²½ìƒ‰:</strong> {background_color}</li>
                        <li><strong>ì„¤ëª…:</strong> ê¸€ì í¬ê¸°ëŠ” í•´ë‹¹ ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ì— ë¹„ë¡€í•©ë‹ˆë‹¤.</li>
                    </ul>
                </div>
            </div>
        </body>
        </html>
        """
        
        return HTMLResponse(content=html_content)
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

